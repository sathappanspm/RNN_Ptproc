#!/usr/bin/env python
#-*- coding:utf-8 -*-

"""
    *.py: Description of what * does.
    Last Modified:
"""

__author__ = "Sathappan Muthiah"
__email__ = "sathap1@vt.edu"
__version__ = "0.0.1"

import tensorflow as tf
from RMTP_PtProc import RMTP, RMTP_TIME
from BatchIterator import PaddedDataIterator
import numpy as np

BATCH_SIZE = 16 # Batch size
ITERS = 30000 # how many generator iterations to train for
REG = 0.1 # tradeoff between time and mark loss
LR = 1e-4 # learning rate
TYPE = "event" # sys.argv[1] # model type: joint event timeseries
DIM=0
SEED = 12345 # set graph-level seed to make the random sequences generated by all ops be repeatable across sessions
tf.set_random_seed(SEED)
np.random.seed(SEED)

with open("event-train.txt") as inf:
    events = np.loadtxt(inf)
    DIM = len(np.unique(events))

with open("time-train.txt") as inf:
    timeVal = np.loadtxt(inf)

data = np.stack([timeVal.T, events.T]).T

event_iterator = PaddedDataIterator(data,0,MARK=True,DIFF=True)

print(DIM)
with tf.Session() as sess:
    rt = RMTP(16, BATCH_SIZE, 2, input_class_size=[1, DIM],
              input_embedding_size=[1, 16], clipping_val=3.0,
              losstype="intensity")
    rt.init_variables()
    rt.build_graph(learning_rate=LR)
    tf.global_variables_initializer().run(session=sess)
    for it in range(ITERS):
        real_batch = event_iterator.next_batch(BATCH_SIZE)
        eseq = real_batch[0]#np.flip(np.abs(real_batch[0]), axis=2)
        seqlen = real_batch[1]
        #print(eseq[0,:, :])
        xin = eseq[:, :-1, :]
        yout = eseq[:, 1:, :]
        rt_args = {"x_in:0": xin, "y_out:0": yout,
                   "lenmask:0": seqlen}

        _, totalloss, timeloss, markerloss = sess.run([rt._train, rt._cost, rt._timeloss, rt._markerloss], feed_dict=rt_args)
        print("total:{:.4}, time:{:.4}, marker: {:.4}".format(totalloss, timeloss, markerloss))


